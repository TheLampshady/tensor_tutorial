{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Artificial Neural Network\n",
    "\n",
    "## Basic\n",
    "* Singal Layer\n",
    "* Optimizer: GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from math import exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download images and labels into mnist.test (10K images+labels) \n",
    "and mnist.train (60K images+labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-labels-idx1-ubyte.gz\nExtracting data/t10k-images-idx3-ubyte.gz\nExtracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import \\\n",
    "    input_data as mnist_data\n",
    "\n",
    "mnist = mnist_data.read_data_sets(\n",
    "    \"data\",\n",
    "    one_hot=True,\n",
    "    reshape=False,\n",
    "    validation_size=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = 28\n",
    "height = 28\n",
    "area = width * height\n",
    "final_nodes = 10\n",
    "\n",
    "lr = .003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Clear Tensor Names. ( Not required)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "Parameters for future functions that will require data to be passed\n",
    "* Input Image: [28, 28, 1] 28px by 28px by 1 channel(grey scale)\n",
    "    * Reshaped to [784] 28 x 28 x 1\n",
    "* Output: [10] prediction form 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, width, height, 1]) \n",
    "Y_ = tf.placeholder(tf.float32, [None, final_nodes])\n",
    "\n",
    "XX = tf.reshape(X, [-1, area])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias (v1)\n",
    "Variables for use by the network. These get initialized at the start\n",
    "and train over time.\n",
    "\n",
    "* Two single arrays to be initialized as zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([area, final_nodes]))\n",
    "B = tf.Variable(tf.zeros([final_nodes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "Requires more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Function (V1)\n",
    "Converts network results to prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.nn.softmax(tf.matmul(XX, W) + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Loss Function (V1-3)\n",
    "Calculates the loss by comparing the tagert and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer (V1-3)\n",
    "Back-propagation function for adjusting weights and biases\n",
    "* Uses Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy (V1-6)\n",
    "% of correct answers found in batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Tensor Graph  (V1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init = tf.initialize_all_variables() # Deprecated\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (V1-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    train_data = {X: batch_X, Y_: batch_Y}\n",
    "\n",
    "    sess.run(train_step, feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test (V1-6)\n",
    "Load batch of test images / correct answers and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9201\n"
     ]
    }
   ],
   "source": [
    "test_data={X: mnist.test.images, Y_: mnist.test.labels}\n",
    "a, _ = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2: Another layer and better starting points\n",
    "This section will add another layer of nodes in the middle (a hidden layer).\n",
    "* Multilayer Perceptron (2 layers)\n",
    "\t* New starting points for weights and biases\n",
    "* Activation function: sigmoid\n",
    "* Optimizer: GradientDescentOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants (v2)\n",
    "Adding a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias (v2)\n",
    "* Adding Layers: The more layers, the more filters to pick up features in your data.\n",
    "* Weights: truncated_normal() is used to provide various starting weights.\n",
    "* Bias: tf.ones() is now used giving the init values an average starting point instead of zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [area, hidden_layer], \n",
    "        stddev=0.1\n",
    "    )\n",
    ")\n",
    "W2 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [hidden_layer, final_nodes], \n",
    "        stddev=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "B1 = tf.Variable(tf.ones([hidden_layer])/10)\n",
    "B2 = tf.Variable(tf.ones([final_nodes])/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function (V2)\n",
    "Activation functions are used on layers to determine the importance of information.\n",
    "\n",
    "Products of nodes with small values have increased chances of being ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.nn.sigmoid(tf.matmul(XX, W1) + B1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Function V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = tf.nn.softmax(tf.matmul(Y, W2) + B2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2a\n",
    "This section will show how to add x amount of layers.\n",
    "\n",
    "**NOTE:** These new layers will cause a drop in accuracy. Why?\n",
    "* Multilayer Perceptron (2 layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants (v3)\n",
    "Dynamic number of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "        area,\n",
    "        200,\n",
    "        100,\n",
    "        60,\n",
    "        30,\n",
    "        final_nodes\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "Assign to Y for looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Bias (v3-5)\n",
    "A list of Weights and Biases that loop through the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "WW = [\n",
    "    tf.Variable(tf.truncated_normal(\n",
    "        [layers[i], layers[i+1]],\n",
    "        stddev=0.1,\n",
    "        name=\"Weights\"\n",
    "    ))\n",
    "    for i in range(len(layers)-1)\n",
    "]\n",
    "\n",
    "BB = [\n",
    "    tf.Variable(tf.ones([layers[i]])/10, \"Biases\")\n",
    "    for i in range(1, len(layers))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function (v2a)\n",
    "Looping Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(layers)-2):\n",
    "    name = \"activate_\" + str(i)\n",
    "    Y = tf.nn.sigmoid(tf.matmul(Y, WW[i], name=name) + BB[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Function (v2a)\n",
    "Formats the output into a format we can use for training against the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.matmul(Y, WW[i+1]) + BB[i+1]\n",
    "Y = tf.nn.softmax(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3: Function Swapping\n",
    "* Multilayer Perceptron (5 layers)\n",
    "* **Activation Function**: relu\n",
    "    * Replaces `sigmoid` with `relu`\n",
    "* **Optimizer**: AdamOptimizer\n",
    "    * replace `GradientDescentOptimiser` with `AdamOptimizer`\n",
    "* **Loss Function**: Use activation results instead of softmax. \n",
    "    * Adjust function to handle 0 which `softmax` never returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = XX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions (v3)\n",
    "Using Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(layers)-2):\n",
    "    preactivate = tf.matmul(Y, WW[i], name=\"Product\") + BB[i]\n",
    "    Y = tf.nn.relu(preactivate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Functions (v3)\n",
    "Break out logits for loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(Y, WW[i+1]) + BB[i+1]\n",
    "Y = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (V3)\n",
    "Loss function based upon Activation and not Regression.\n",
    "\n",
    "**NOTE** Fixes the issue where the tf.log function tries to compute 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_)\n",
    "cross_entropy = tf.reduce_mean(logits) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer (V3)\n",
    "Tensorflow has many optimizers and `AdamOptimizer` works well with large dimensional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4: Learning Rates\n",
    "* Multilayer Perceptron (5 layers)\n",
    "* **Dynamic Learning Rate**: reduces as time goes on. (from .003 to 0.00001)\n",
    "* Activation Function: relu\n",
    "* Optimizer: AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants (v4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmax = 0.003\n",
    "lrmin = 0.00001\n",
    "decay_speed = 2000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders (V4)\n",
    "For Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training (V5-6)\n",
    "# - Learning rate decreases as time goes on.\n",
    "\n",
    "for i in range(1000):\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    learning_rate = lrmin + (lrmax - lrmin) * exp(-i / decay_speed)\n",
    "    train_data = {X: batch_X, Y_: batch_Y, L: learning_rate}\n",
    "\n",
    "    # train\n",
    "    sess.run(train_step, feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 6: Dropoff\n",
    "* Multilayer Perceptron (5 layers)\n",
    "* Drop-off: 90% change of keeping a node for training\n",
    "    * Prevents over fitting (The network could find unrelated data important)\n",
    "* Learning Rate: Dynamically reduces as time goes on. (from .003 to 0.00001)\n",
    "* Activation Function: relu\n",
    "* Optimizer: AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants (V5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_ratio = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders (V5)\n",
    "For dropoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a4fb56624bd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions (v5)\n",
    "Turns off some nodes. Prevents false positives from being piked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = XX\n",
    "i = 0\n",
    "for i in range(len(layers)-2):\n",
    "    name = \"activate_\" + str(i)\n",
    "    Y = tf.nn.relu(tf.matmul(Y, WW[i], name=name) + BB[i])\n",
    "    Y = tf.nn.dropout(Y, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training (V6)\n",
    "Turns off some nodes. Prevents false positives from being piked up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)\n",
    "    learning_rate = lrmin + (lrmax - lrmin) * exp(-i / decay_speed)\n",
    "    train_data = {\n",
    "        X: batch_X,\n",
    "        Y_: batch_Y,\n",
    "        L: learning_rate,\n",
    "        keep_prob: keep_ratio\n",
    "    }\n",
    "\n",
    "    sess.run(train_step, feed_dict=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {X: mnist.test.images, Y_: mnist.test.labels, keep_prob: 1.0}\n",
    "a,c = sess.run([accuracy, cross_entropy], feed_dict=test_data)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
